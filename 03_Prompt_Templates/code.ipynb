{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts\n",
    "\n",
    "Prompt allow you to create **System Messages with input variables**, for example, this:\n",
    "**SystemMessage(content=\"You are a helpful assistant that translates the English to Spanish.\")**\n",
    "English and Spanish may be dynamic. This can be archieved with templates\n",
    "\n",
    "- system template: enable to one-to-one input and output pairs \n",
    "- prompt template: enable to create more dynamic pairs that input and output could be dynamic, such as languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic input and output\n",
    "1. set the template for system message where input and output could be flexibly adjusted for user's desire\n",
    "2. template is only useful when combining with a LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = \"\"\"\n",
    "You are a helpful assistant that translates the {input_language} to {output_language}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. use PromptTemplate from Langchain to call different language usage later on\n",
    "3. it can greatly fast the usage convienence instead of enter the language in system message from time to time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYou are a helpful assistant that translates the english to chinese\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=TEMPLATE\n",
    ")\n",
    "prompt_template.format(input_language=\"english\", output_language=\"chinese\") # dynamic prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing the input_variables to the constructor will provide additional validation for the template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYou are a helpful assistant that translates the english to chinese\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = PromptTemplate(template=TEMPLATE, input_variables=[\"input_language\", \"output_language\"])\n",
    "prompt_template.format(input_language=\"english\", output_language=\"chinese\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering Basics\n",
    "1. **Zero-shot prompting**\n",
    "- when you ask a LM to perform a task **without giving it any examples**.\n",
    "- it relies only on its **pre-existing knowledge** and understanding.\n",
    "\n",
    "2. **Few-shot prompting**\n",
    "- provide a LM with a small # of examples to help it to **understand a task or context**, guiding its response or output\n",
    "\n",
    "3. **Chain-of-Thought prompting**\n",
    "- give LM a series of examples along **with reasoning or thought process behind them**.\n",
    "- help LM to understand **not just 'What' but 'why' behind decision or classifications**\n",
    "\n",
    "**Suggestions**\n",
    "- Use zero-shot prompting as a baseline\n",
    "- if the result or classification is not desired, then we can use few-shot prompting\n",
    "- if few-shot prompting doesn't work, then chain-of-thought prompting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few Shot Prompt - provide a few examples in the template\n",
    "- Few shot prompt enable to provide LLM with a few examples in the templates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero-shot prompting\n",
    "\n",
    "1. there is only one input for user query\n",
    "2. text:{input} \n",
    "3. only use the pre-existing knowledge without giving any knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is for internal evaluation review template\n",
    "\n",
    "TEMPLATE = \"\"\"\n",
    "Interprete the text and evaluate the text.\n",
    "sentiment: is the text in a positive, neutral or negative sentiment?\n",
    "subject: What subject is the text about? Use exactly one word.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "sentiment\n",
    "subject\n",
    "\n",
    "text: {input}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot prompting\n",
    "\n",
    "- To improve performance we can provide examples to increase the quality of the output.\n",
    "- provide examples with sentiment for guidance and instruct LM what is positive, negative, or neutral sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = \"\"\"\n",
    "Interprete the text and evaluate the text.\n",
    "sentiment: is the text in a positive, neutral or negative sentiment?\n",
    "subject: What subject is the text about? Use exactly one word.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "sentiment\n",
    "subject\n",
    "\n",
    "text: {input}\n",
    "\n",
    "Examples:\n",
    "text: The BellaVista restaurant offers an exquisite dining experience. The flavors are rich and the presentation is impeccable.\n",
    "sentiment: positive\n",
    "subject: BellaVista\n",
    "\n",
    "text: BellaVista restaurant was alright. The food was decent, but nothing stood out.\n",
    "sentiment: neutral\n",
    "subject: BellaVista\n",
    "\n",
    "text: I was disappointed with BellaVista. The service was slow and the dishes lacked flavor.\n",
    "sentiment: negative\n",
    "subject: BellaVista\n",
    "\n",
    "text: SeoulSavor offered the most authentic Korean flavors I've tasted outside of Seoul. The kimchi was perfectly fermented and spicy.\n",
    "sentiment: positive\n",
    "subject: SeoulSavor\n",
    "\n",
    "text: SeoulSavor was okay. The bibimbap was good but the bulgogi was a bit too sweet for my taste.\n",
    "sentiment: neutral\n",
    "subject: SeoulSavor\n",
    "\n",
    "text: I didn't enjoy my meal at SeoulSavor. The tteokbokki was too mushy and the service was not attentive.\n",
    "sentiment: negative\n",
    "subject: SeoulSavor\n",
    "\n",
    "text: MunichMeals has the best bratwurst and sauerkraut I've tasted outside of Bavaria. Their beer garden ambiance is truly authentic.\n",
    "sentiment: positive\n",
    "subject: MunichMeals\n",
    "\n",
    "text: MunichMeals was alright. The weisswurst was okay, but I've had better elsewhere.\n",
    "sentiment: neutral\n",
    "subject: MunichMeals\n",
    "\n",
    "text: I was let down by MunichMeals. The potato salad lacked flavor and the staff seemed uninterested.\n",
    "sentiment: negative\n",
    "subject: MunichMeals\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nInterprete the text and evaluate the text.\\nsentiment: is the text in a positive, neutral or negative sentiment?\\nsubject: What subject is the text about? Use exactly one word.\\n\\nFormat the output as JSON with the following keys:\\nsentiment\\nsubject\\n\\ntext: The MunichDeals experience was just awesome!\\n\\nExamples:\\ntext: The BellaVista restaurant offers an exquisite dining experience. The flavors are rich and the presentation is impeccable.\\nsentiment: positive\\nsubject: BellaVista\\n\\ntext: BellaVista restaurant was alright. The food was decent, but nothing stood out.\\nsentiment: neutral\\nsubject: BellaVista\\n\\ntext: I was disappointed with BellaVista. The service was slow and the dishes lacked flavor.\\nsentiment: negative\\nsubject: BellaVista\\n\\ntext: SeoulSavor offered the most authentic Korean flavors I've tasted outside of Seoul. The kimchi was perfectly fermented and spicy.\\nsentiment: positive\\nsubject: SeoulSavor\\n\\ntext: SeoulSavor was okay. The bibimbap was good but the bulgogi was a bit too sweet for my taste.\\nsentiment: neutral\\nsubject: SeoulSavor\\n\\ntext: I didn't enjoy my meal at SeoulSavor. The tteokbokki was too mushy and the service was not attentive.\\nsentiment: negative\\nsubject: SeoulSavor\\n\\ntext: MunichMeals has the best bratwurst and sauerkraut I've tasted outside of Bavaria. Their beer garden ambiance is truly authentic.\\nsentiment: positive\\nsubject: MunichMeals\\n\\ntext: MunichMeals was alright. The weisswurst was okay, but I've had better elsewhere.\\nsentiment: neutral\\nsubject: MunichMeals\\n\\ntext: I was let down by MunichMeals. The potato salad lacked flavor and the staff seemed uninterested.\\nsentiment: negative\\nsubject: MunichMeals\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the prompt template with dynamic input variable \"input\" that appears in the template (text: {input})\n",
    "prompt_template = PromptTemplate(template=TEMPLATE, input_variables=[\"input\"])\n",
    "\n",
    "# Based on the prompt template, format the prompt with the input text\n",
    "prompt_template.format(input=\"The MunichDeals experience was just awesome!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative few-shot prompting\n",
    "\n",
    "- LangChain also provides a FewShotPromptTemplate class, which allows creating the examples more modularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"text\": \"The BellaVista restaurant offers an exquisite dining experience. The flavors are rich and the presentation is impeccable.\",\n",
    "        \"response\": \"sentiment: positive\\nsubject: BellaVista\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"BellaVista restaurant was alright. The food was decent, but nothing stood out.\",\n",
    "        \"response\": \"sentiment: neutral\\nsubject: BellaVista\"\n",
    "    },\n",
    "    ### other examples are left out\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can even append the new example to the example created before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_example = {\n",
    "    \"text\": \"SeoulSavor was okay. The bibimbap was good but the bulgogi was a bit too sweet for my taste.\",\n",
    "    \"response\": \"sentiment: neutral\\nsubject: SeoulSavor\"\n",
    "}\n",
    "examples.append(new_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- use normal prompt template class and provide input variable with text and responses \n",
    "- create template inside the prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = PromptTemplate(input_variables=[\"text\", \"response\"], template=\"Text: {text}\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create input as user query once few shot template is created.\n",
    "- normally we can pass multiple inputs inside input_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"text: {input}\",\n",
    "    input_variables=[\"input\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The BellaVista restaurant offers an exquisite dining experience. The flavors are rich and the presentation is impeccable.\n",
      "sentiment: positive\n",
      "subject: BellaVista\n",
      "\n",
      "Text: BellaVista restaurant was alright. The food was decent, but nothing stood out.\n",
      "sentiment: neutral\n",
      "subject: BellaVista\n",
      "\n",
      "Text: SeoulSavor was okay. The bibimbap was good but the bulgogi was a bit too sweet for my taste.\n",
      "sentiment: neutral\n",
      "subject: SeoulSavor\n",
      "\n",
      "text: The MunichDeals experience was just awesome!\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(input=\"The MunichDeals experience was just awesome!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain-of-thought Prompting\n",
    "\n",
    "- Instead of just providing examples, we can also provide examples which include the whole thought process of why a review is negative, neutral or positive\n",
    "- it share the same structure of few-shot template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = \"\"\"\n",
    "Interprete the text and evaluate the text. Determine if the text has a positive, neutral, or negative sentiment. Also, identify the subject of the text in one word.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "sentiment\n",
    "subject\n",
    "\n",
    "text: {input}\n",
    "\n",
    "Chain-of-Thought Prompts:\n",
    "Let's start by evaluating a statement. Consider: \"The BellaVista restaurant offers an exquisite dining experience. The flavors are rich and the presentation is impeccable.\" How does this make you feel about BellaVista?\n",
    " It sounds like a positive review for BellaVista.\n",
    "\n",
    "Based on the positive nature of that statement, how would you format your response?\n",
    " { \"sentiment\": \"positive\", \"subject\": \"BellaVista\" }\n",
    "\n",
    "Now, think about this: \"SeoulSavor was okay. The bibimbap was good but the bulgogi was a bit too sweet for my taste.\" Does this give a strong feeling either way?\n",
    " Not particularly. It seems like a mix of good and not-so-good elements, so it's neutral.\n",
    "\n",
    "Given the neutral sentiment, how should this be presented?\n",
    " { \"sentiment\": \"neutral\", \"subject\": \"SeoulSavor\" }\n",
    "\n",
    "Lastly, ponder on this: \"I was let down by MunichMeals. The potato salad lacked flavor and the staff seemed uninterested.\" What's the overall impression here?\n",
    " The statement is expressing disappointment and dissatisfaction.\n",
    "\n",
    "And if you were to categorize this impression, what would it be?\n",
    " { \"sentiment\": \"negative\", \"subject\": \"MunichMeals\" }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composition\n",
    "\n",
    "You can also compose multiple prompts together. This can be especially useful if you want to reuse parts of prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yd/0rx8hpn96fd0480f9ndhwn_00000gn/T/ipykernel_51052/2695125543.py:41: LangChainDeprecationWarning: This class is deprecated. Please see the docstring below or at the link for a replacement option: https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.pipeline.PipelinePromptTemplate.html\n",
      "  pipeline_prompt = PipelinePromptTemplate(final_prompt=full_prompt, pipeline_prompts=input_prompts)\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# Introduction\n",
    "introduction_template = \"\"\"\n",
    "Interprete the text and evaluate the text. Determine if the text has a positive, neutral, or negative sentiment. Also, identify the subject of the text in one word.\n",
    "\"\"\"\n",
    "introduction_prompt = PromptTemplate.from_template(introduction_template)\n",
    "\n",
    "# Example\n",
    "example_template = \"\"\"\n",
    "Chain-of-Thought Prompts:\n",
    "Let's start by evaluating a statement. Consider: \"{example_text}\". How does this make you feel about {example_subject}?\n",
    "Response: {example_evaluation}\n",
    "\n",
    "Based on the {example_sentiment} nature of that statement, how would you format your response?\n",
    "Response: {example_format}\n",
    "\"\"\"\n",
    "example_prompt = PromptTemplate.from_template(example_template)\n",
    "\n",
    "# Execution\n",
    "execution_template = \"\"\"\n",
    "Now, execute this process for the text: \"{input}\".\n",
    "\"\"\"\n",
    "execution_prompt = PromptTemplate.from_template(execution_template)\n",
    "\n",
    "# Composing the full prompt\n",
    "full_template = \"\"\"{introduction}\n",
    "\n",
    "{example}\n",
    "\n",
    "{execution}\"\"\"\n",
    "full_prompt = PromptTemplate.from_template(full_template)\n",
    "\n",
    "# PipelinePrompts\n",
    "input_prompts = [\n",
    "    (\"introduction\", introduction_prompt),\n",
    "    (\"example\", example_prompt),\n",
    "    (\"execution\", execution_prompt)\n",
    "]\n",
    "pipeline_prompt = PipelinePromptTemplate(final_prompt=full_prompt, pipeline_prompts=input_prompts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Interprete the text and evaluate the text. Determine if the text has a positive, neutral, or negative sentiment. Also, identify the subject of the text in one word.\n",
      "\n",
      "\n",
      "\n",
      "Chain-of-Thought Prompts:\n",
      "Let's start by evaluating a statement. Consider: \"The BellaVista restaurant offers an exquisite dining experience. The flavors are rich and the presentation is impeccable.\". How does this make you feel about BellaVista?\n",
      "Response: It sounds like a positive review for BellaVista.\n",
      "\n",
      "Based on the positive nature of that statement, how would you format your response?\n",
      "Response: { \"sentiment\": \"positive\", \"subject\": \"BellaVista\" }\n",
      "\n",
      "\n",
      "\n",
      "Now, execute this process for the text: \"The new restaurant downtown has bland dishes and the wait time is too long.\".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pipeline_prompt.format(\n",
    "    example_text=\"The BellaVista restaurant offers an exquisite dining experience. The flavors are rich and the presentation is impeccable.\",\n",
    "    example_subject=\"BellaVista\",\n",
    "    example_evaluation=\"It sounds like a positive review for BellaVista.\",\n",
    "    example_sentiment=\"positive\",\n",
    "    example_format='{ \"sentiment\": \"positive\", \"subject\": \"BellaVista\" }',\n",
    "    input=\"The new restaurant downtown has bland dishes and the wait time is too long.\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serializing prompts\n",
    "\n",
    "- Example is an easy prompt since serializing **does not work for the PipelinePromptTemplate** (yet)\n",
    "- serilize the template and save it to the disk\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(input_variables=[\"input\"], template=\"Tell me a joke about {input}\") # very basic prompt template\n",
    "prompt.save(\"prompt.yaml\") # easy to read by user \n",
    "prompt.save(\"prompt.json\") # easy to read by machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a joke about chickens'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "\n",
    "prompt = load_prompt(\"prompt.yaml\") # get out the prompt saved above cell\n",
    "prompt.format(input=\"chickens\") # input query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a joke about cows'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = load_prompt(\"prompt.json\")\n",
    "prompt.format(input=\"cows\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
